{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-01T21:34:43.995216Z",
     "start_time": "2018-09-01T21:34:30.862745Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier, LinearRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "import functions as fu\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "%pylab inline\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from sklearn.decomposition import NMF, TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn import ensemble\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-01T21:34:45.077525Z",
     "start_time": "2018-09-01T21:34:44.015226Z"
    }
   },
   "outputs": [],
   "source": [
    "dataTrainNormalized = joblib.load('DUMP_reviews_normalized.pkl')\n",
    "labels = joblib.load('DUMP_labels_normalized.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-01T21:34:45.083530Z",
     "start_time": "2018-09-01T21:34:45.079528Z"
    }
   },
   "outputs": [],
   "source": [
    "RS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-01T21:34:45.114550Z",
     "start_time": "2018-09-01T21:34:45.086031Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54486"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make decision: tranfer 1-5 rating to 0-1 rating\n",
    "labels_=[]\n",
    "for idx in range (0, len(labels)):\n",
    "    labels_.append(1 if labels[idx]>3 else 0)\n",
    "labels = labels_\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T21:37:45.679438Z",
     "start_time": "2018-08-25T21:37:45.383688Z"
    }
   },
   "source": [
    "## Looking for model with different ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-09-01T21:35:29.353Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      ">>>>>>>>c трансформером\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.8687185821935701\n",
      "\n",
      "\n",
      "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
      "        max_iter=None, normalize=False, random_state=1, solver='auto',\n",
      "        tol=0.001)\n",
      "0.8647910152477477\n",
      "\n",
      "\n",
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=1, tol=0.0001,\n",
      "     verbose=0)\n",
      "0.8635062899736203\n",
      "\n",
      "\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "0.7378408930361511\n",
      "\n",
      "\n",
      "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "0.8497962544995599\n",
      "\n",
      "\n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
      "       n_jobs=-1, penalty='l2', power_t=0.5, random_state=1, shuffle=True,\n",
      "       tol=None, verbose=0, warm_start=False)\n",
      "0.8676173549376314\n",
      "\n",
      "\n",
      ">>>>>>>>без трансформера\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.854586518287413\n",
      "\n",
      "\n",
      "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
      "        max_iter=None, normalize=False, random_state=1, solver='auto',\n",
      "        tol=0.001)\n",
      "0.8305619564283614\n",
      "\n",
      "\n",
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=1, tol=0.0001,\n",
      "     verbose=0)\n",
      "0.8359396051918637\n",
      "\n",
      "\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "0.8526961368848027\n",
      "\n",
      "\n",
      "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "0.8497962544995599\n",
      "\n",
      "\n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
      "       n_jobs=-1, penalty='l2', power_t=0.5, random_state=1, shuffle=True,\n",
      "       tol=None, verbose=0, warm_start=False)\n",
      "0.8314244579110295\n",
      "\n",
      "\n",
      ">>>>>>>>------------------------------------\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      ">>>>>>>>c трансформером\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.8664060881522412\n",
      "\n",
      "\n",
      "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
      "        max_iter=None, normalize=False, random_state=1, solver='auto',\n",
      "        tol=0.001)\n",
      "0.8774731303826873\n",
      "\n",
      "\n",
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=1, tol=0.0001,\n",
      "     verbose=0)\n",
      "0.8791616348297989\n",
      "\n",
      "\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "0.7134126109487504\n",
      "\n",
      "\n",
      "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "0.7327019986761709\n",
      "\n",
      "\n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
      "       n_jobs=-1, penalty='l2', power_t=0.5, random_state=1, shuffle=True,\n",
      "       tol=None, verbose=0, warm_start=False)\n",
      "0.8612487761199047\n",
      "\n",
      "\n",
      ">>>>>>>>без трансформера\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.8725177121471741\n",
      "\n",
      "\n",
      "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
      "        max_iter=None, normalize=False, random_state=1, solver='auto',\n",
      "        tol=0.001)\n",
      "0.8470616006638043\n",
      "\n",
      "\n",
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=1, tol=0.0001,\n",
      "     verbose=0)\n",
      "0.8625885691565323\n",
      "\n",
      "\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "0.8074367667634712\n",
      "\n",
      "\n",
      "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "0.7327019986761709\n",
      "\n",
      "\n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
      "       n_jobs=-1, penalty='l2', power_t=0.5, random_state=1, shuffle=True,\n",
      "       tol=None, verbose=0, warm_start=False)\n",
      "0.8598538799539955\n",
      "\n",
      "\n",
      ">>>>>>>>------------------------------------\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      ">>>>>>>>c трансформером\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.8584774435637537\n",
      "\n",
      "\n",
      "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
      "        max_iter=None, normalize=False, random_state=1, solver='auto',\n",
      "        tol=0.001)\n",
      "0.8771244004467464\n",
      "\n",
      "\n",
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=1, tol=0.0001,\n",
      "     verbose=0)\n",
      "0.8785376065005357\n",
      "\n",
      "\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "0.7126601287859444\n",
      "\n",
      "\n",
      "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "0.7125316597952098\n",
      "\n",
      "\n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
      "       n_jobs=-1, penalty='l2', power_t=0.5, random_state=1, shuffle=True,\n",
      "       tol=None, verbose=0, warm_start=False)\n",
      "0.8484931839707709\n",
      "\n",
      "\n",
      ">>>>>>>>без трансформера\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.8723157914042318\n",
      "\n",
      "\n",
      "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
      "        max_iter=None, normalize=False, random_state=1, solver='auto',\n",
      "        tol=0.001)\n",
      "0.8525125580282568\n",
      "\n",
      "\n",
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=1, tol=0.0001,\n",
      "     verbose=0)\n",
      "0.8654883824923452\n",
      "\n",
      "\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7611680219694403\n",
      "\n",
      "\n",
      "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "0.7125316597952098\n",
      "\n",
      "\n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
      "       n_jobs=-1, penalty='l2', power_t=0.5, random_state=1, shuffle=True,\n",
      "       tol=None, verbose=0, warm_start=False)\n",
      "0.8641485204062874\n",
      "\n",
      "\n",
      ">>>>>>>>------------------------------------\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 4), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      ">>>>>>>>c трансформером\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.8526961503578623\n",
      "\n",
      "\n",
      "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
      "        max_iter=None, normalize=False, random_state=1, solver='auto',\n",
      "        tol=0.001)\n",
      "0.8749036478359159\n",
      "\n",
      "\n",
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=1, tol=0.0001,\n",
      "     verbose=0)\n",
      "0.8771977932543187\n",
      "\n",
      "\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "0.712531656426945\n",
      "\n",
      "\n",
      "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "0.7123481280943722\n",
      "\n",
      "\n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
      "       n_jobs=-1, penalty='l2', power_t=0.5, random_state=1, shuffle=True,\n",
      "       tol=None, verbose=0, warm_start=False)\n",
      "0.835223720911096\n",
      "\n",
      "\n",
      ">>>>>>>>без трансформера\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.8705722124580901\n",
      "\n",
      "\n",
      "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
      "        max_iter=None, normalize=False, random_state=1, solver='auto',\n",
      "        tol=0.001)\n",
      "0.8538523578014143\n",
      "\n",
      "\n",
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=1, tol=0.0001,\n",
      "     verbose=0)\n"
     ]
    }
   ],
   "source": [
    "# it works a lot of time. Manually looking thru prints results\n",
    "for currentVectorizer in [CountVectorizer(ngram_range=(1,1)),\n",
    "                          CountVectorizer(ngram_range=(1,2)),\n",
    "                          CountVectorizer(ngram_range=(1,3)),\n",
    "                          CountVectorizer(ngram_range=(1,4)),\n",
    "                          CountVectorizer(ngram_range=(1,5))]:\n",
    "#with transform\n",
    "    print (currentVectorizer)\n",
    "    print (\">>>>>>>>with transform\")\n",
    "    for clf in [LogisticRegression (random_state=RS, n_jobs = -1), \n",
    "            RidgeClassifier (random_state=RS),\n",
    "            LinearSVC(random_state=RS), \n",
    "            MultinomialNB(),\n",
    "            BernoulliNB(),\n",
    "            SGDClassifier(n_jobs = -1, random_state=RS)#, \n",
    "            #ensemble.RandomForestClassifier(n_estimators = 500, min_samples_split=5, random_state=RS, n_jobs = -1),\n",
    "            #xgb.XGBClassifier(n_estimators=500,  random_state=RS, n_jobs = -1)\n",
    "               ]:\n",
    "        print (clf)\n",
    "        print (cross_val_score(fu.get_pipeline3(currentVectorizer, TfidfTransformer(), clf), dataTrainNormalized, labels, cv = 5, n_jobs = -1).mean())\n",
    "        print (\"\\n\")\n",
    "    \n",
    "#with no transform\n",
    "    print (\">>>>>>>>with no transform\")\n",
    "    for clf in [LogisticRegression (random_state=RS, n_jobs = -1), \n",
    "            RidgeClassifier (random_state=RS),\n",
    "            LinearSVC(random_state=RS), \n",
    "            MultinomialNB(),\n",
    "            BernoulliNB(),\n",
    "            SGDClassifier(n_jobs = -1, random_state=RS)#, \n",
    "            #ensemble.RandomForestClassifier(n_estimators = 500, min_samples_split=5, random_state=RS, n_jobs = -1),\n",
    "            #xgb.XGBClassifier(n_estimators=500,  random_state=RS, n_jobs = -1)\n",
    "               ]:\n",
    "        print (clf)\n",
    "        print (cross_val_score(fu.get_pipeline2(currentVectorizer, clf), dataTrainNormalized, labels, cv = 5, n_jobs = -1).mean())\n",
    "        print (\"\\n\")\n",
    "    \n",
    "       \n",
    "    print (\">>>>>>>>------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
